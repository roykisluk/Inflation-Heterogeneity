{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Parameters, Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "## Years\n",
    "start_year = 2019\n",
    "end_year = 2022\n",
    "base_year = start_year\n",
    "years = range(start_year, end_year + 1)\n",
    "\n",
    "## Grouping\n",
    "young_age_cutoff=25\n",
    "old_age_threshold=65\n",
    "\n",
    "## Indexing\n",
    "price_variable = 'mehir' # 'mehir' or 'omdan'\n",
    "\n",
    "## Output\n",
    "top_n = 5\n",
    "comparison_year = end_year\n",
    "comparison_level = 'primary'\n",
    "\n",
    "## Folder Names\n",
    "cex_data_folder=\"/Users/roykisluk/Downloads/Consumer_Expenditure_Survey/\"\n",
    "folder_names_pathname='Data_clean/CEX_folder_names.csv'\n",
    "age_groups_pathname='Data_clean/age_groups.csv'\n",
    "prodcode_dict_pathname = 'Data_clean/prodcode_dictionary_c3-c399.csv'\n",
    "    \n",
    "## Libraries\n",
    "import pandas as pd\n",
    "import pyreadstat  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load folder names\n",
    "folder_names_df = pd.read_csv(folder_names_pathname)\n",
    "\n",
    "# Load age groups\n",
    "age_groups_df = pd.read_csv(age_groups_pathname)\n",
    "young_age_group_id = age_groups_df[(age_groups_df['min_age'] <= young_age_cutoff) & (age_groups_df['max_age'] >= young_age_cutoff)].index[0] + 1\n",
    "old_age_group_id = age_groups_df[(age_groups_df['min_age'] <= old_age_threshold) & (age_groups_df['max_age'] >= old_age_threshold)].index[0] + 1\n",
    "\n",
    "# Load household data for each year\n",
    "dfs_mb = {}\n",
    "for year in years:\n",
    "    subfolder = folder_names_df.loc[folder_names_df['Year'] == year, 'Folder_Name'].values[0]\n",
    "    data_HH_pathname = f\"{cex_data_folder}{subfolder}/{subfolder}datamb.sas7bdat\"\n",
    "    df, meta = pyreadstat.read_sas7bdat(data_HH_pathname)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    if 'gil' in df.columns:\n",
    "        df.rename(columns={'gil': 'age_group'}, inplace=True)\n",
    "    dfs_mb[year] = df\n",
    "\n",
    "# Load individual data for each year\n",
    "dfs_prat = {}\n",
    "for year in years:\n",
    "    subfolder = folder_names_df.loc[folder_names_df['Year'] == year, 'Folder_Name'].values[0]\n",
    "    data_IND_pathname = f\"{cex_data_folder}{subfolder}/{subfolder}dataprat.sas7bdat\"\n",
    "    df, meta = pyreadstat.read_sas7bdat(data_IND_pathname)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    if 'gil' in df.columns:\n",
    "        df.rename(columns={'gil': 'age_group'}, inplace=True)\n",
    "    dfs_prat[year] = df\n",
    "\n",
    "# Load survey data for each year\n",
    "dfs_survey = {}\n",
    "for year in years:\n",
    "    subfolder = folder_names_df.loc[folder_names_df['Year'] == year, 'Folder_Name'].values[0]\n",
    "    data_prices_pathname = f\"{cex_data_folder}{subfolder}/{subfolder}datayoman.sas7bdat\"\n",
    "    df, meta = pyreadstat.read_sas7bdat(data_prices_pathname)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    dfs_survey[year] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Groups = {}\n",
    "for year in years:\n",
    "    Groups[year] = pd.DataFrame(dfs_mb[year]['misparmb'].unique(), columns=['misparmb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    dfs_mb_year = dfs_mb[year]\n",
    "    dfs_prat_year = dfs_prat[year]\n",
    "\n",
    "    nationality_map = {1: 'Jewish', 2: 'Arab'}\n",
    "    observance_map = {1: 'Secular', 2: 'Conservative', 3: 'Religious', 4: 'Ultra-Orthodox', 5: 'Mixed'}\n",
    "\n",
    "    Groups[year]['Nationality'] = dfs_mb_year['nationality'].map(nationality_map).fillna('Other')\n",
    "    Groups[year]['Observance'] = dfs_mb_year['ramatdatiyut'].map(observance_map).fillna('Other')\n",
    "\n",
    "    age_group_map = {age_group_id: 'Young' if age_group_id <= young_age_group_id else 'Old' if age_group_id >= old_age_group_id else 'Middle' for age_group_id in dfs_prat_year['age_group'].unique()}\n",
    "    Groups[year]['Age_Group'] = dfs_prat_year.loc[dfs_prat_year['y_kalkali'] == 1, 'age_group'].map(age_group_map).values\n",
    "\n",
    "    Groups[year]['Income_Decile'] = dfs_mb_year['decile'].fillna(0).astype(int)\n",
    "\n",
    "    Groups[year]['Income_Quintile'] = pd.cut(dfs_mb_year['decile'], bins=[0, 2, 4, 6, 8, 10], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "    Groups[year]['SES_Quintile'] = dfs_mb_year['cluster'].apply(lambda x: x if x in range(1, 6) else np.nan).fillna(0).astype(int)\n",
    "    Groups[year]['SES_Tertile'] = dfs_mb_year['cluster'].apply(lambda x: 1 if x in [1, 2] else 2 if x == 3 else 3 if x in [4, 5] else np.nan).fillna(0).astype(int)\n",
    "\n",
    "    Groups[year]['Children'] = dfs_mb_year['nefashotad18'].fillna(0).astype(int)\n",
    "    Groups[year]['Family_Size'] = Groups[year]['Children'].apply(lambda x: 'no children' if x == 0 else '1 to 3' if x in [1, 2, 3] else '4 plus')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groups dataframes headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    display(HTML(f\"<h2>Groups for Year {year}</h2>\"))\n",
    "    display(HTML(Groups[year].head().to_html(index=False)))\n",
    "    print(f\"Number of observations: {len(dfs_mb[year])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot groups distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns to plot\n",
    "columns_to_plot = [col for col in Groups[end_year].columns if col != 'misparmb']\n",
    "\n",
    "# Calculate the number of rows needed\n",
    "ncols = 3\n",
    "nrows = (len(columns_to_plot) + ncols - 1) // ncols\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each column\n",
    "for ax, column in zip(axes, columns_to_plot):\n",
    "    Groups[end_year][column].value_counts().sort_index().plot(kind='bar', ax=ax, color='skyblue')\n",
    "    ax.set_title(f'Distribution of {column} in {end_year}')\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for ax in axes[len(columns_to_plot):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_consumption = 0.0\n",
    "for j in range(0, len(df)):\n",
    "    total_consumption += df['omdan'][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "def total_consumption_value(df): \n",
    "\n",
    "    return total_consumption\n",
    "\n",
    "def keep_shared_prodcodes(df1, df2):\n",
    "    shared_prodcodes = set(df1['prodcode']).intersection(set(df2['prodcode']))\n",
    "    df1_shared = df1[df1['prodcode'].isin(shared_prodcodes)].reset_index(drop=True)\n",
    "    df2_shared = df2[df2['prodcode'].isin(shared_prodcodes)].reset_index(drop=True)\n",
    "    return df1_shared, df2_shared\n",
    "\n",
    "def weighting(df):\n",
    "    weights = pd.DataFrame(df['prodcode'].unique(), columns=['prodcode'])\n",
    "    weights['weight'] = 0.0\n",
    "    total_consumption = total_consumption_value(df)\n",
    "    for j in range(0, len(weights)):\n",
    "        weights.loc[j, 'weight'] = df[df['prodcode'] == weights.loc[j, 'prodcode']]['omdan'].sum() / total_consumption\n",
    "    return weights\n",
    "\n",
    "def average_price(df):\n",
    "    average_prices = pd.DataFrame(df['prodcode'].unique(), columns=['prodcode'])\n",
    "    average_prices['price'] = 0.0\n",
    "    for j in range(0, len(average_prices)):\n",
    "        average_prices.loc[j, 'price'] = (df[df['prodcode'] == average_prices.loc[j, 'prodcode']]['mehir'] / df[df['prodcode'] == average_prices.loc[j, 'prodcode']]['kamut']).mean()\n",
    "    return average_prices\n",
    "\n",
    "def Laspeyres(df_base, df_current):\n",
    "    index_df = pd.DataFrame(df_base['prodcode'].unique(), columns=['prodcode'])\n",
    "    index_df['index'] = 0.0\n",
    "    weights = weighting(df_base)\n",
    "    average_prices_base = average_price(df_base)\n",
    "    average_prices_current = average_price(df_current)\n",
    "    index_df = index_df.merge(weights, on='prodcode', how='left')\n",
    "    index_df = index_df.merge(average_prices_base, on='prodcode', how='left', suffixes=('', '_base'))\n",
    "    index_df = index_df.merge(average_prices_current, on='prodcode', how='left', suffixes=('_base', '_current'))\n",
    "    total_index = 0.0\n",
    "    missing_base_prices = 0\n",
    "    missing_current_prices = 0\n",
    "    for j in range(len(index_df)):\n",
    "        price_current = index_df.loc[j, 'price_current']\n",
    "        price_base = index_df.loc[j, 'price_base']\n",
    "        if price_base == 0 or pd.isna(price_base) or np.isinf(price_base):\n",
    "            index_df.loc[j, 'index'] = factor * 100\n",
    "            missing_base_prices += 1\n",
    "            continue\n",
    "        if price_current == 0 or pd.isna(price_current) or np.isinf(price_current):\n",
    "            index_df.loc[j, 'index'] = factor * 100\n",
    "            missing_current_prices += 1\n",
    "            continue\n",
    "        index_df.loc[j, 'index'] = (price_current / price_base) * 100\n",
    "    for j in range(len(index_df)):\n",
    "        weight = index_df.loc[j, 'weight']\n",
    "        total_index += weight * index_df.loc[j, 'index']\n",
    "    return index_df, total_index\n",
    "\n",
    "def merge_to_secondary(df):\n",
    "    df['prodcode_secondary'] = df['prodcode'].astype(str).str[:3]\n",
    "    grouped = df.groupby('prodcode_secondary', group_keys=False).apply(\n",
    "        lambda x: pd.Series({\n",
    "            'price_index': np.average(x['index'], weights=x['weight']) if x['weight'].sum() > 0 else np.nan,\n",
    "            'total_weight': x['weight'].sum()\n",
    "        }),\n",
    "        include_groups=False \n",
    "    ).reset_index()\n",
    "    grouped.rename(columns={'prodcode_secondary': 'prodcode'}, inplace=True)\n",
    "    grouped.rename(columns={'total_weight': 'weight'}, inplace=True)\n",
    "    return grouped\n",
    "\n",
    "def merge_to_primary(df):\n",
    "    df['prodcode_primary'] = df['prodcode'].astype(str).str[:2]\n",
    "    grouped = df.groupby('prodcode_primary', group_keys=False).apply(\n",
    "        lambda x: pd.Series({\n",
    "            'price_index': np.average(x['price_index'], weights=x['weight']) if x['weight'].sum() > 0 else np.nan,\n",
    "            'total_weight': x['weight'].sum()\n",
    "        }),\n",
    "        include_groups=False\n",
    "    ).reset_index()\n",
    "    grouped.rename(columns={'prodcode_primary': 'prodcode'}, inplace=True)\n",
    "    grouped.rename(columns={'total_weight': 'weight'}, inplace=True)\n",
    "    return grouped\n",
    "\n",
    "\n",
    "\n",
    "# Filter observations for relevant group\n",
    "if group_mmb is not None:\n",
    "    for year in years:\n",
    "        dfs_survey[year] = dfs_survey[year][dfs_survey[year]['misparmb'].isin(group_mmb[year]['misparmb'])]\n",
    "\n",
    "# Filter observations with prodcode that starts with 3\n",
    "for year in years:\n",
    "    dfs_survey[year] = dfs_survey[year][dfs_survey[year]['prodcode'].astype(str).str.startswith('3')].reset_index(drop=True)\n",
    "\n",
    "# Calculate weights and price indexes\n",
    "yearly_price_index = {}\n",
    "df_price_index = {}\n",
    "for year in years:\n",
    "    df_base, df_current = keep_shared_prodcodes(dfs_survey[base_year], dfs_survey[year])\n",
    "    df_price_index[year], yearly_price_index[year] = Laspeyres(df_base, df_current)\n",
    "\n",
    "# Combine all years into a single dataframe\n",
    "combined_df = pd.concat(df_price_index.values(), keys=df_price_index.keys(), names=['Year', 'Index']).reset_index(level='Index', drop=True).reset_index()\n",
    "combined_df = combined_df[['Year', 'prodcode', 'index', 'weight']]\n",
    "\n",
    "# Merge to secondary and primary categories\n",
    "df_secondary = {}\n",
    "df_primary = {}\n",
    "for year in years:\n",
    "    df_secondary[year] = merge_to_secondary(df_price_index[year])\n",
    "    df_primary[year] = merge_to_primary(df_secondary[year])\n",
    "\n",
    "# Combine secondary and primary categories into a single dataframe\n",
    "combined_secondary_df = pd.concat(df_secondary.values(), keys=df_secondary.keys(), names=['Year', 'Index']).reset_index(level='Index', drop=True).reset_index()\n",
    "combined_primary_df = pd.concat(df_primary.values(), keys=df_primary.keys(), names=['Year', 'Index']).reset_index(level='Index', drop=True).reset_index()\n",
    "\n",
    "# Keep only the necessary columns\n",
    "combined_secondary_df = combined_secondary_df[['Year', 'prodcode', 'price_index', 'weight']]\n",
    "combined_primary_df = combined_primary_df[['Year', 'prodcode', 'price_index', 'weight']]\n",
    "\n",
    "# Load prodcode dictionary\n",
    "prodcode_dict_df = pd.read_csv(prodcode_dict_pathname)\n",
    "\n",
    "# Remove description column if it already exists\n",
    "if 'description' in combined_secondary_df.columns:\n",
    "    combined_secondary_df = combined_secondary_df.drop(columns=['description'])\n",
    "if 'description' in combined_primary_df.columns:\n",
    "    combined_primary_df = combined_primary_df.drop(columns=['description'])\n",
    "\n",
    "# Convert prodcode to string in both dataframes before merging\n",
    "prodcode_dict_df['prodcode'] = prodcode_dict_df['prodcode'].astype(str)\n",
    "combined_secondary_df['prodcode'] = combined_secondary_df['prodcode'].astype(str)\n",
    "\n",
    "# Merge descriptions into combined_secondary_df\n",
    "combined_secondary_df = combined_secondary_df.merge(prodcode_dict_df, on='prodcode', how='left')\n",
    "\n",
    "# Merge descriptions into combined_primary_df\n",
    "combined_primary_df = combined_primary_df.merge(prodcode_dict_df, on='prodcode', how='left')\n",
    "\n",
    "return combined_df, combined_secondary_df, combined_primary_df, yearly_price_index\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
